---
title: "622 GROUP PROJECT 3"
author: "Jessica Valencia"
date: "2023-03-31"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

```{r}
# Tidyverse and text analysis tools
library(tidyverse)
library(tidytext)
library(dplyr)

# For decision trees
library(rpart)
library(rpart.plot)
library(rattle)

# For nearest neighbors
library(class)

# For ML evaluation
library(caret)
```

#INTRODUCTION########################################

In this assignment we first chose the random 200 observation from our data set that we previously cleaned and dealt with in assignment 2. After selecting our 200 observations, we then proceeded by conducting a hand coded sentiment analysis. This task was split between all four contributors in this project. After conducting a hand coded sentiment analysis, we then proceeded to import the data and then decided to do some cleaning from the human error that was present after the hand coding. We decided to remove the "neutral" sentiments and were left with 99  observations. With these observations we proceeded to conduct tokenization and ML models to look at trends in the data. 


#RESULTS SUMMARY#########################

After cleaning our data and preparing it for tokenization, we decided to remove stop words in order to get a better look at requiring words related to the sentiment of our data. We found that the most reoccurring words were "shooting","msu",and "michigan" among other words. This shows how our data is relevant considering our topic is Michigan State University and the shooting that occured there about a month ago. After determining what words were being frequently shown in our data, we then looked into the distribution of the word count and found that words that appeared less than 10 times were unnecessary within our data analysis, as these words were probably irrelevant to our topic of interest. we then created a column with the top most frequent words, which included words such as "prayers", "students", and "police". We created some fitted models, K-Nearest Neighbors and Decision Tree models, in order to look at the quality of our data. We found that between these two models our accuracy remained at around 73%, which we found to be good results for our data. We then verified our values by looping through models. 

First, we brought in the data file after hand coded sentiment has been conducted.
```{r}
library(readr)
datasubset <- read_csv("hw3datasubset.csv")
#View(hw3datasubset)
```
#REMOVING NEUTRAL SENTIMENT FROM HANDCODED SENTIMENT##############


Then, we removed the observations with a "neutral" hand-coded sentiment from the data set considering that this information is not useful for the purpose of this report. 
```{r}
data_new<-datasubset[datasubset$Handcoded_Sentiment!="Neutral",]

View(data_new)
```

#CLEANING HANDCODED SENTIMENT##############


We decided to clean the hand-coded sentiment column in order to make sure our results are as optimal and accurate as possible by removing human error. 
```{r}
data_new$Handcoded_Sentiment <- replace(data_new$Handcoded_Sentiment, data_new$Handcoded_Sentiment == "Posotive", "Positive")

data_new$Handcoded_Sentiment <- replace(data_new$Handcoded_Sentiment, data_new$Handcoded_Sentiment == "Postive", "Positive")

data_new$Handcoded_Sentiment <- replace(data_new$Handcoded_Sentiment, data_new$Handcoded_Sentiment == "postive", "Positive")
```


#TOKENIZATION###########################

-**Tokenize:** Split up the abstracts into individual words

```{r}
if ("dplyr" %in% installed.packages()[, "Package"]){ 
  cat("'dplyr' is installed.")
} else {
  install.packages("dplyr",dependencies=T)
}
library(dplyr)
```

```{r}
if ("tokenizers" %in% installed.packages()[, "Package"]){ 
  cat("'tokenizers' is installed.")
} else {
  install.packages("tokenizers",dependencies=T)
}
library(tokenizers)

```


```{r}
library(dplyr)
library(tokenizers)
library(ggplot2)
data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>%  #tokenize
  dplyr::count(word, sort = TRUE) %>%  # count by word
  dplyr::arrange(desc(n)) %>%  # Everything from this point on is just to graph
  head(20) %>% 
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat= 'identity') + 
  coord_flip()
```
First, we look at the frequency of the top 20 words among all the observations by conducting tokenization. As we can see the most frequent words detected are words such as "the","to", and "and" which are considered stop words. Our goal is to focus on the words that determine the sentiment of the observations, therefore we will need to remove these stop words from our results in order to focus on the words that determine the sentiment. 

```{r}
data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>%  # tokenize
  anti_join(stop_words)%>% # Remove stop words
  count(word, sort = TRUE) %>% # count by word
  arrange(desc(n)) %>% # Everything from this point on is just to graph
  head(20) %>% 
  ggplot(aes(x = reorder(word, n), y = n)) +
  geom_bar(stat= 'identity') + 
  coord_flip()
```
After removing the stop words, as we can see, the removal of stopwords allows words like shooting, police, gun to appear as frequent words.It is evident that "shooting" is the most apparent word with over 100 appearances among the data set. This word is followed by "msu" with over 75 appearances.

Now, we will look at the distribution of word counts by using a histogram on a log scale. This will show us the number of words that appear only once compared to those that appear frequently throughout the data set.
```{r}
data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>%  # tokenize
  anti_join(stop_words)%>% # Remove stop words
  count(word, sort = TRUE) %>%
  ggplot(aes(n)) +
  geom_histogram() +
  scale_x_log10()
```
According to the results shown above, there are a lot of words that appear only once or infrequently , which we remove in the next step. We chose 10 as the cut off for the number of times words reappear. The rationale behind analyzing word frequency is that if these words are extremely rare they are likely to be less informative and may contain errors. 
 
Once we were able to determine the infrequent words in the data set, we proceeded to remove infrequent words. 
```{r}
data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>%  # tokenize
  anti_join(stop_words)%>% # Remove stop words
  count(word, sort = TRUE) %>% # count by word
  filter(n >= 10) %>%  # Remove words that occur less than 10 times
  ggplot(aes(n)) +
  geom_histogram() +
  scale_x_log10()
```
As shown above, we decided to remove words that appear less than 10 times in the data set. This will help us narrow in on the important and necessary data.



## Creating Features
With the information acquired thus far , we decided to create a column vector of words that only appear more than ten times. We are doing so because words that appear less than 10 times are most likely stop words it may contain a lot of typos with unnecessary information. This will help us determine the important words within our data set and will help build our ML models.
```{r}
word_list <- data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>% 
  anti_join(stop_words)%>% 
  count(word, sort = TRUE) %>%
  filter(n >= 10)%>%
  pull(word)

word_list
```

Then, we decided to determine the count of the important words from the word list that each "title text" column contains. This will help us determine the observations that contain the most relevant information and will help build our ML models.
```{r}
select_features <- data_new %>% 
  tidytext::unnest_tokens(word, 'title_text') %>% 
  anti_join(stop_words)%>%            #remove stop words
  filter(word %in% word_list) %>%   #filter for only words in the wordlist
  count(timestamp, word) %>%          #count word useage by abstract
  spread(word, n) %>%                 #convert to wide format
  map_df(replace_na, 0) #replace NA with 0 to create dataset

#select_features
```

We then joined this back with our original dataset, using timestamp, to get the labels matched up with our features.
```{r}
full_data <- data_new %>% 
  right_join(select_features, by = 'timestamp') %>% 
  select(-SentimentGI,-text, -NegativityGI,-NegativityLM,-NegativityHE,-PositivityGI,-SentimentHE,-PositivityHE,-PositivityLM,-SentimentLM,-LC,-Vader) # Remove extra variables
head(full_data)
```

## Train and Test Split
Full data contains the timestamp variable (which we will use to make our train/test split in the next section), as well as our features (each word) and the Handcoded Sentiment.
```{r}
full_data <- full_data %>% select(3,18:50)
# 30% holdout sample
test <- full_data %>% sample_frac(.3)

# Rest in the training set
train <- full_data %>% anti_join(test, by = 'timestamp') %>% select(-timestamp)

# Remove ID after using to create train/test
test <- test %>% select(-timestamp)
```

# Fitting Models
We decided to proceed with K-Nearest Neighbors and Decision Trees for our ML models as these give a lot of valuable information about our data.

## First attempt at a model
We first started with K-Nearest Neighbors model. This simply checks the class of closest k neighbors, and takes a vote of them to predict what the class of the data point will be. We can fit this model using the `class` package. This will help us have the actual and predicted values in one data frame which will beuseful for us in the following steps.
```{r}
# Create separate training and testing features and labels objects
train_features <- train %>% select(-Handcoded_Sentiment)
test_features <- test %>% select(-Handcoded_Sentiment)

train_label <- train$Handcoded_Sentiment
test_label <- test$Handcoded_Sentiment

# Predicted values from K-NN, with K = 11
knnpred <- knn(train_features,test_features,train_label, k = 11)
```

```{r}
pred_actual <- data.frame(predicted = knnpred, actual = test_label)
pred_actual %>% head()
```
Now that we have the predicted and actual values in one data frame, we can create a confusion matrix and evaluate how well our model is performing. 
```{r}
pred_actual %>% table()
confusionMatrix(pred_actual %>% table(), positive = 'Positive')

```
As shown in our results, we have an accuracy of 0.7667 which we fins to be high and a good indication of our data set. However, our recall results is at 0.0 and our precision is not determined which we find to be interesting and something worth exploring in the future. 

## Running a Decision Tree model
Now, we decided to look at a decision tree model to get a better understanding of our data and see what other information this model can give us. 
```{r}
treemod <- rpart(Handcoded_Sentiment ~ ., 
                 data = train, 
                 method = 'class', 
                 control = rpart.control(minsplit = 25))
```

```{r}
fancyRpartPlot(treemod, sub = "")
```

### Evaluating the Model
After creating the decision tree model, we must test it to see the accuracy of our results. We will use the predict function to make predictions on new data and compare the predicted values to the actual values.
```{r}
pred <- predict(treemod, test)
head(pred)
```

```{r}
test_pred <- data.frame(score = pred[,2], actual = test$Handcoded_Sentiment)
head(test_pred)
```

```{r}
test_pred <- test_pred %>% arrange(desc(score))
test_pred$pred <- 'Negative'

top_scores <- floor(nrow(test_pred)*0.2)
test_pred$pred[1:top_scores] <- 'Positive'
```

```{r}
pred_table <- test_pred %>% select(pred, actual) %>% table()
confusionMatrix(pred_table, positive = 'Positive')

precision(pred_table, relevant = 'Positive')
recall(pred_table, relevant = 'Positive')
```
Based on our results, the accuracy of our model is 0.7333, the recall is evaluated at 0.375 and our precision is evaluated at 0.5 . Compared to our previous model, the accuracy seems pretty comparable considering the values are close in range. However, we found that it was interesting how recall is significantly higher and precision was able to be determined. 

### Looping through models
In order to verify our values and get more information, we decided to loop through many different decision tree models to compare values and see if our values are comparable to different version of the decision tree model. 
```{r}
# We will look at minsplit values of 5, 10, 15, 20
splits <- c(5,10,15,20)

# We'll look at maxdepths of 2, 3, 4, 5
depths <- c(2,3,4,5)

# We'll consider predicting the top 5%, 10%, and 20% as cell biology
percent <- c(.05, .1, .2)

# How many different models are we running?
nmods <- length(splits)*length(depths)*length(percent)

# We will store results in this data frame
results <- data.frame(splits = rep(NA,nmods), 
                      depths = rep(NA, nmods),
                      percent = rep(NA,nmods),
                      precision = rep(NA,nmods),
                      recall = rep(NA,nmods))

# The model number that we will iterate on (aka models run so far)
mod_num <- 1
```

```{r, eval=FALSE}
# The loop
for(i in 1:length(splits)){
  for(j in 1:length(depths)){
    s <- splits[i]
    d <- depths[j]
    # Running the model
    treemod <- rpart(Handcoded_Sentiment ~ ., 
                     data = train, 
                     method = 'class',
                     control = rpart.control(minsplit = s, maxdepth = d))
      
    # Find the predictions
    pred <- predict(treemod, test)
      
    # Attach scores to the test set
    # Then sort by descending order
    test_pred <- data.frame(score = pred[,2], actual = test$Handcoded_Sentiment)
    test_pred <- test_pred %>% arrange(desc(score))
      
    # Make predictions based on scores
    # We loop through each threshold value here.
    for(k in 1:length(percent)){
      p <- percent[k]
      
      # Predict the top % as True
      test_pred$pred <- 'Negative '
      top_scores <- floor(nrow(test_pred)*p)
      test_pred$pred[1:top_scores] <- 'Positive'
      
      # Confusion Matrix
      pred_tab <- test_pred %>% select(pred, actual) %>% table()
      
      # Store results
      results[mod_num,] <- c(s, 
                             d,
                             p,
                             precision(pred_tab, relevant = 'Positive'), 
                             recall(pred_tab, relevant = 'Positive'))
      # Increment the model number
      mod_num <- mod_num + 1
    }
  }
}

# All results are stored in the "results" dataframe
head(results)

# Best recall? Top 5 in descending order
results %>% arrange(desc(recall)) %>% head()

# Best precision? Top 5 in descending order
results %>% arrange(desc(precision)) %>% head()
```
## Simplifying the Process with Caret
We've already used the `caret` package for tools like the confusion matrix and precision/recall. However, we can also use it to do other validation methods more easily, such as k-fold cross validation. Here, we'll look at a quick example of using `caret` to find the training and test sets need to do 10-fold cross validation.

```{r, eval=FALSE}
# Create a list of 10 folds (each element has indices of the fold)
flds <- createFolds(full_data$timestamp, k = 10, list = TRUE, returnTrain = FALSE)
str(flds)
# Create train and test using fold 1 as test
patent_test01 <- full_data[flds$Fold01,]
patent_train01 <- full_data[-flds$Fold01,]
```

we created a list of 10 vectors, each containing a fold. So, to do our 10-fold cross validation

```{r, eval=FALSE}
library(caret)

# Create 10 folds
flds <- createFolds(full_data$timestamp, k = 10, list = TRUE, returnTrain = FALSE)

# Initialize vectors to store the evaluation metrics
accuracy_vec <- numeric(length(flds))
precision_vec <- numeric(length(flds))
recall_vec <- numeric(length(flds))

# Loop through each fold
for (i in seq_along(flds)) {
  # Split the data into train and test sets using the current fold
  test <- full_data[flds[[i]], ]
  train <- full_data[-flds[[i]], ]
  
  # Train the model on the training set
  model <- train(Handcoded_Sentiment ~ ., data = train, method = "rf")
  
  # Make predictions on the test set
  pred <- predict(model, newdata = test)
  
  # Calculate evaluation metrics
  confusion_matrix <- confusionMatrix(data = pred, reference = test$Handcoded_Sentiment)
  accuracy_vec[i] <- confusion_matrix$overall["Accuracy"]
  precision_vec[i] <- confusion_matrix$byClass["Precision"]
  recall_vec[i] <- confusion_matrix$byClass["Recall"]
}

# Calculate the mean and standard deviation of the evaluation metrics across the 10 folds
mean_accuracy <- mean(accuracy_vec)
sd_accuracy <- sd(accuracy_vec)
mean_precision <- mean(precision_vec)
sd_precision <- sd(precision_vec)
mean_recall <- mean(recall_vec)
sd_recall <- sd(recall_vec)

# Print the results
cat(paste0("Mean accuracy: ", round(mean_accuracy, 3), "\n"))
cat(paste0("Standard deviation of accuracy: ", round(sd_accuracy, 3), "\n"))
cat(paste0("Mean precision: ", round(mean_precision, 3), "\n"))
cat(paste0("Standard deviation of precision: ", round(sd_precision, 3), "\n"))
cat(paste0("Mean recall: ", round(mean_recall, 3), "\n"))
cat(paste0("Standard deviation of recall: ", round(sd_recall, 3), "\n"))

```

